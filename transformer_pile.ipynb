{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b98dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content')\n",
    "from transformer import Transformer\n",
    "from autoencoder import SparseAutoencoder\n",
    "from data_utils import get_batch_iterator\n",
    "import torch, h5py, os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vocab_size': 50304,\n",
    "    'context_length': 128,\n",
    "    'n_embed': 128,\n",
    "    'n_head': 8,\n",
    "    'train_path': 'data/med_pile_train.h5',\n",
    "    'dev_path': 'data/pile_val.h5',\n",
    "    't_out_path': 'models/transformer_full.pt',\n",
    "    'a_out_path': 'models/autoencoder.pt',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_small_batch_iterator(data_path, batch_size, context_length, device='cpu'):\n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        tokens = f['tokens'][:]\n",
    "    n_examples = (len(tokens) - 1) // context_length\n",
    "    indices = np.arange(n_examples)\n",
    "    def generator():\n",
    "        while True:\n",
    "            np.random.shuffle(indices)\n",
    "            for i in range(0, n_examples - batch_size + 1, batch_size):\n",
    "                batch_idx = indices[i:i + batch_size]\n",
    "                samples = torch.tensor(np.array([\n",
    "                    tokens[j * context_length: j * context_length + context_length + 1] for j in batch_idx\n",
    "                ])).long()\n",
    "                xb = samples[:, :-1].to(device)\n",
    "                yb = samples[:, 1:].to(device)\n",
    "                yield xb, yb\n",
    "    return generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = config['device']\n",
    "model = Transformer(\n",
    "    n_head=config['n_head'],\n",
    "    n_embed=config['n_embed'],\n",
    "    context_length=config['context_length'],\n",
    "    vocab_size=config['vocab_size']\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "iterator = get_small_batch_iterator(config['train_path'], batch_size=8, context_length=128, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "pbar = tqdm(range(500))\n",
    "for step in pbar:\n",
    "    xb, yb = next(iterator)\n",
    "    _, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    pbar.set_description(f\"Step {step} | Loss {np.mean(losses[-50:]):.4f}\")\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict()\n",
    "}, config['t_out_path'])\n",
    "print(\"Transformer saved to:\", config['t_out_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0593234",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = SparseAutoencoder(\n",
    "    n_features=512,\n",
    "    n_embed=config['n_embed']\n",
    ").to(device)\n",
    "optimizer_ae = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b492483",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_losses = []\n",
    "reg_losses = []\n",
    "pbar = tqdm(range(200))\n",
    "for _ in pbar:\n",
    "    xb, _ = next(iterator)\n",
    "    with torch.no_grad():\n",
    "        x_embed, _ = model.forward_embedding(xb)\n",
    "    rand_idx = torch.randint(config['context_length'], (xb.shape[0],))\n",
    "    embed_samples = x_embed[range(xb.shape[0]), rand_idx, :]\n",
    "    flat_embed = embed_samples.view(xb.shape[0], -1)\n",
    "    optimizer_ae.zero_grad()\n",
    "    _, recon_loss, reg_loss = autoencoder(flat_embed, compute_loss=True)\n",
    "    loss = recon_loss + 3e-3 * reg_loss\n",
    "    loss.backward()\n",
    "    optimizer_ae.step()\n",
    "    autoencoder.normalize_decoder_weights()\n",
    "    recon_losses.append(recon_loss.item())\n",
    "    reg_losses.append(reg_loss.item())\n",
    "    pbar.set_description(f\"Recon: {np.mean(recon_losses[-50:]):.3f}, Reg: {np.mean(reg_losses[-50:]):.3f}\")\n",
    "\n",
    "torch.save({'model_state_dict': autoencoder.state_dict()}, config['a_out_path'])\n",
    "print(\"Autoencoder saved to:\", config['a_out_path'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
