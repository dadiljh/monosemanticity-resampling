{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from transformers import GPT2Tokenizer"
      ],
      "metadata": {
        "id": "nNVDrnkqfPBa"
      },
      "id": "nNVDrnkqfPBa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "U42wpB_vfPcB"
      },
      "id": "U42wpB_vfPcB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is transforming the world.\",\n",
        "    \"Mechanistic interpretability helps us understand models.\",\n",
        "    \"Language models can memorize and generalize.\",\n",
        "    \"Transformers are the foundation of modern NLP.\",\n",
        "] * 100"
      ],
      "metadata": {
        "id": "-eAQYsb4fUBv"
      },
      "id": "-eAQYsb4fUBv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "for text in texts:\n",
        "    encoded = tokenizer.encode(text, add_special_tokens=False)\n",
        "    tokens.extend(encoded)\n",
        "tokens = np.array(tokens, dtype=np.uint16)\n",
        "print(\"Total tokens:\", len(tokens))"
      ],
      "metadata": {
        "id": "Bp9njK1bfV6A"
      },
      "id": "Bp9njK1bfV6A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with h5py.File(\"data/med_pile_train.h5\", \"w\") as f:\n",
        "    f.create_dataset(\"tokens\", data=tokens)\n",
        "\n",
        "with h5py.File(\"data/pile_val.h5\", \"w\") as f:\n",
        "    f.create_dataset(\"tokens\", data=tokens[: len(tokens) // 10])"
      ],
      "metadata": {
        "id": "_dsbQTcCfYDq"
      },
      "id": "_dsbQTcCfYDq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}